{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd8ff95",
   "metadata": {},
   "source": [
    "# Breast cancer molecular subtyping using hypergraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the feature reduction that slects the best features using ramdom forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7712d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABCB8', 'ABHD13', 'ABL2', 'ACSL4', 'ACSM2A', 'ACTG1', 'ACVRL1', 'ADA',\n",
      "       'ADCY4', 'ADCY7',\n",
      "       ...\n",
      "       'ZNF592', 'ZNF608', 'ZNF645', 'ZNF652', 'ZNF740', 'ZNF75A', 'ZNF770',\n",
      "       'ZNF830', 'ZNRF2', 'ZSCAN29'],\n",
      "      dtype='object', length=1000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('cnv_old.csv')\n",
    "y = pd.read_csv('labels_old.csv')                  \n",
    "# x = data.drop('SAMPLE_ID',axis = 1,inplace = True) \n",
    "# x = data.drop('SUBTYPE',axis = 1,inplace = True)\n",
    "x=data\n",
    "x = x.astype(np.float64)\n",
    "np.ravel(y,order='c')\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100),max_features=1000)\n",
    "sel.fit(x_train, y_train)\n",
    "sel.get_support()\n",
    "selected_feat= x_train.columns[(sel.get_support())]\n",
    "print(selected_feat)\n",
    "methx_1=x[selected_feat]\n",
    "savetxt('cnv_old_1000.csv', methx_1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the deep embedded clustering that groups the cancer dataset into subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa01394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 23\n",
      "Epoch 1/100\n",
      "980/980 [==============================] - 0s 428us/step - loss: 9.6722e-04\n",
      "Epoch 2/100\n",
      "980/980 [==============================] - 0s 366us/step - loss: 8.1265e-04\n",
      "Epoch 3/100\n",
      "980/980 [==============================] - 0s 364us/step - loss: 6.2189e-04\n",
      "Epoch 4/100\n",
      "980/980 [==============================] - 0s 384us/step - loss: 4.5907e-04\n",
      "Epoch 5/100\n",
      "980/980 [==============================] - 0s 372us/step - loss: 3.3854e-04\n",
      "Epoch 6/100\n",
      "980/980 [==============================] - 0s 402us/step - loss: 2.5531e-04\n",
      "Epoch 7/100\n",
      "980/980 [==============================] - 0s 389us/step - loss: 1.9988e-04\n",
      "Epoch 8/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 1.6369e-04\n",
      "Epoch 9/100\n",
      "980/980 [==============================] - 0s 381us/step - loss: 1.4027e-04\n",
      "Epoch 10/100\n",
      "980/980 [==============================] - 0s 358us/step - loss: 1.2531e-04\n",
      "Epoch 11/100\n",
      "980/980 [==============================] - 0s 351us/step - loss: 1.1576e-04\n",
      "Epoch 12/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 1.0970e-04\n",
      "Epoch 13/100\n",
      "980/980 [==============================] - 0s 361us/step - loss: 1.0584e-04\n",
      "Epoch 14/100\n",
      "980/980 [==============================] - 0s 369us/step - loss: 1.0343e-04\n",
      "Epoch 15/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 1.0188e-04\n",
      "Epoch 16/100\n",
      "980/980 [==============================] - 0s 356us/step - loss: 1.0092e-04\n",
      "Epoch 17/100\n",
      "980/980 [==============================] - 0s 364us/step - loss: 1.0031e-04\n",
      "Epoch 18/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 9.9919e-05\n",
      "Epoch 19/100\n",
      "980/980 [==============================] - 0s 359us/step - loss: 9.9681e-05\n",
      "Epoch 20/100\n",
      "980/980 [==============================] - 0s 360us/step - loss: 9.9528e-05\n",
      "Epoch 21/100\n",
      "980/980 [==============================] - 0s 370us/step - loss: 9.9430e-05\n",
      "Epoch 22/100\n",
      "980/980 [==============================] - 0s 362us/step - loss: 9.9369e-05\n",
      "Epoch 23/100\n",
      "980/980 [==============================] - 0s 370us/step - loss: 9.9331e-05\n",
      "Epoch 24/100\n",
      "980/980 [==============================] - 0s 361us/step - loss: 9.9306e-05\n",
      "Epoch 25/100\n",
      "980/980 [==============================] - 0s 366us/step - loss: 9.9291e-05\n",
      "Epoch 26/100\n",
      "980/980 [==============================] - 0s 348us/step - loss: 9.9282e-05\n",
      "Epoch 27/100\n",
      "980/980 [==============================] - 0s 369us/step - loss: 9.9275e-05\n",
      "Epoch 28/100\n",
      "980/980 [==============================] - 0s 376us/step - loss: 9.9272e-05\n",
      "Epoch 29/100\n",
      "980/980 [==============================] - 0s 368us/step - loss: 9.9269e-05\n",
      "Epoch 30/100\n",
      "980/980 [==============================] - 0s 362us/step - loss: 9.9268e-05\n",
      "Epoch 31/100\n",
      "980/980 [==============================] - 0s 361us/step - loss: 9.9269e-05\n",
      "Epoch 32/100\n",
      "980/980 [==============================] - 0s 362us/step - loss: 9.9266e-05\n",
      "Epoch 33/100\n",
      "980/980 [==============================] - 0s 381us/step - loss: 9.9266e-05\n",
      "Epoch 34/100\n",
      "980/980 [==============================] - 0s 362us/step - loss: 9.9264e-05\n",
      "Epoch 35/100\n",
      "980/980 [==============================] - 0s 372us/step - loss: 9.9263e-05\n",
      "Epoch 36/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 9.9264e-05\n",
      "Epoch 37/100\n",
      "980/980 [==============================] - 0s 383us/step - loss: 9.9264e-05\n",
      "Epoch 38/100\n",
      "980/980 [==============================] - 0s 375us/step - loss: 9.9264e-05\n",
      "Epoch 39/100\n",
      "980/980 [==============================] - 0s 373us/step - loss: 9.9263e-05\n",
      "Epoch 40/100\n",
      "980/980 [==============================] - 0s 384us/step - loss: 9.9263e-05\n",
      "Epoch 41/100\n",
      "980/980 [==============================] - 0s 380us/step - loss: 9.9264e-05\n",
      "Epoch 42/100\n",
      "980/980 [==============================] - 0s 385us/step - loss: 9.9264e-05\n",
      "Epoch 43/100\n",
      "980/980 [==============================] - 0s 379us/step - loss: 9.9265e-05\n",
      "Epoch 44/100\n",
      "980/980 [==============================] - 0s 372us/step - loss: 9.9263e-05\n",
      "Epoch 45/100\n",
      "980/980 [==============================] - 0s 354us/step - loss: 9.9262e-05\n",
      "Epoch 46/100\n",
      "980/980 [==============================] - 0s 397us/step - loss: 9.9263e-05\n",
      "Epoch 47/100\n",
      "980/980 [==============================] - 0s 367us/step - loss: 9.9263e-05\n",
      "Epoch 48/100\n",
      "980/980 [==============================] - 0s 373us/step - loss: 9.9263e-05\n",
      "Epoch 49/100\n",
      "980/980 [==============================] - 0s 366us/step - loss: 9.9261e-05\n",
      "Epoch 50/100\n",
      "980/980 [==============================] - 0s 399us/step - loss: 9.9263e-05\n",
      "Epoch 51/100\n",
      "980/980 [==============================] - 0s 387us/step - loss: 9.9262e-05\n",
      "Epoch 52/100\n",
      "980/980 [==============================] - 0s 384us/step - loss: 9.9261e-05\n",
      "Epoch 53/100\n",
      "980/980 [==============================] - 0s 349us/step - loss: 9.9262e-05\n",
      "Epoch 54/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 9.9262e-05\n",
      "Epoch 55/100\n",
      "980/980 [==============================] - 0s 370us/step - loss: 9.9261e-05\n",
      "Epoch 56/100\n",
      "980/980 [==============================] - 0s 365us/step - loss: 9.9262e-05\n",
      "Epoch 57/100\n",
      "980/980 [==============================] - 0s 364us/step - loss: 9.9262e-05\n",
      "Epoch 58/100\n",
      "980/980 [==============================] - 0s 363us/step - loss: 9.9262e-05\n",
      "Epoch 59/100\n",
      "980/980 [==============================] - 0s 364us/step - loss: 9.9260e-05\n",
      "Epoch 60/100\n",
      "980/980 [==============================] - 0s 360us/step - loss: 9.9263e-05\n",
      "Epoch 61/100\n",
      "980/980 [==============================] - 0s 355us/step - loss: 9.9261e-05\n",
      "Epoch 62/100\n",
      "980/980 [==============================] - 0s 378us/step - loss: 9.9261e-05\n",
      "Epoch 63/100\n",
      "980/980 [==============================] - 0s 363us/step - loss: 9.9261e-05\n",
      "Epoch 64/100\n",
      "980/980 [==============================] - 0s 352us/step - loss: 9.9260e-05\n",
      "Epoch 65/100\n",
      "980/980 [==============================] - 0s 358us/step - loss: 9.9260e-05\n",
      "Epoch 66/100\n",
      "980/980 [==============================] - 0s 365us/step - loss: 9.9262e-05\n",
      "Epoch 67/100\n",
      "980/980 [==============================] - 0s 350us/step - loss: 9.9261e-05\n",
      "Epoch 68/100\n",
      "980/980 [==============================] - 0s 385us/step - loss: 9.9262e-05\n",
      "Epoch 69/100\n",
      "980/980 [==============================] - 0s 364us/step - loss: 9.9261e-05\n",
      "Epoch 70/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 9.9260e-05\n",
      "Epoch 71/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 9.9260e-05\n",
      "Epoch 72/100\n",
      "980/980 [==============================] - 0s 391us/step - loss: 9.9260e-05\n",
      "Epoch 73/100\n",
      "980/980 [==============================] - 0s 367us/step - loss: 9.9260e-05\n",
      "Epoch 74/100\n",
      "980/980 [==============================] - 0s 378us/step - loss: 9.9259e-05\n",
      "Epoch 75/100\n",
      "980/980 [==============================] - 0s 366us/step - loss: 9.9260e-05\n",
      "Epoch 76/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 9.9259e-05\n",
      "Epoch 77/100\n",
      "980/980 [==============================] - 0s 369us/step - loss: 9.9259e-05\n",
      "Epoch 78/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 9.9260e-05\n",
      "Epoch 79/100\n",
      "980/980 [==============================] - 0s 379us/step - loss: 9.9260e-05\n",
      "Epoch 80/100\n",
      "980/980 [==============================] - 0s 384us/step - loss: 9.9261e-05\n",
      "Epoch 81/100\n",
      "980/980 [==============================] - 0s 373us/step - loss: 9.9261e-05\n",
      "Epoch 82/100\n",
      "980/980 [==============================] - 0s 378us/step - loss: 9.9259e-05\n",
      "Epoch 83/100\n",
      "980/980 [==============================] - 0s 372us/step - loss: 9.9260e-05\n",
      "Epoch 84/100\n",
      "980/980 [==============================] - 0s 376us/step - loss: 9.9259e-05\n",
      "Epoch 85/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 9.9260e-05\n",
      "Epoch 86/100\n",
      "980/980 [==============================] - 0s 391us/step - loss: 9.9258e-05\n",
      "Epoch 87/100\n",
      "980/980 [==============================] - 0s 374us/step - loss: 9.9259e-05\n",
      "Epoch 88/100\n",
      "980/980 [==============================] - 0s 370us/step - loss: 9.9258e-05\n",
      "Epoch 89/100\n",
      "980/980 [==============================] - 0s 373us/step - loss: 9.9259e-05\n",
      "Epoch 90/100\n",
      "980/980 [==============================] - 0s 357us/step - loss: 9.9259e-05\n",
      "Epoch 91/100\n",
      "980/980 [==============================] - 0s 368us/step - loss: 9.9259e-05\n",
      "Epoch 92/100\n",
      "980/980 [==============================] - 0s 361us/step - loss: 9.9258e-05\n",
      "Epoch 93/100\n",
      "980/980 [==============================] - 0s 369us/step - loss: 9.9260e-05\n",
      "Epoch 94/100\n",
      "980/980 [==============================] - 0s 392us/step - loss: 9.9259e-05\n",
      "Epoch 95/100\n",
      "980/980 [==============================] - 0s 382us/step - loss: 9.9259e-05\n",
      "Epoch 96/100\n",
      "980/980 [==============================] - 0s 372us/step - loss: 9.9258e-05\n",
      "Epoch 97/100\n",
      "980/980 [==============================] - 0s 369us/step - loss: 9.9258e-05\n",
      "Epoch 98/100\n",
      "980/980 [==============================] - 0s 376us/step - loss: 9.9258e-05\n",
      "Epoch 99/100\n",
      "980/980 [==============================] - 0s 371us/step - loss: 9.9260e-05\n",
      "Epoch 100/100\n",
      "980/980 [==============================] - 0s 366us/step - loss: 9.9259e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dassen Sathan\\anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\Dassen Sathan\\anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_label  0.0 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import silhouette_score\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "class ClusteringLayer(Layer):\n",
    "    '''\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        ''' \n",
    "        student t-distribution, as used in t-SNE algorithm.\n",
    "        It measures the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "       \n",
    "        inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        \n",
    "        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        '''\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
    "        \n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected symmetric auto-encoder model.\n",
    "  \n",
    "    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n",
    "          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n",
    "\n",
    "    act: activation function\n",
    "    \n",
    "    return:\n",
    "        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    \n",
    "    input_data = Input(shape=(dims[0],), name='input')\n",
    "    x = input_data\n",
    "    \n",
    "    # internal layers of encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # latent hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers of decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # decoder output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n",
    "    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n",
    "    \n",
    "    return autoencoder_model, encoder_model\n",
    "\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n",
    "\n",
    "np.random.seed(10)\n",
    "data = pd.read_csv('newcnv_1000.csv')\n",
    "data.fillna(value=0, inplace = True)\n",
    "data = data.astype(np.float64)\n",
    "scaler = MinMaxScaler() \n",
    "numeric_columns = data.columns.values.tolist()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "#autoencoder\n",
    "\n",
    "n_clusters = 5# 3 - 0.291, 4 - 0.280, 5 - 0.228, 6 - 0.227, 10 - 0.192\n",
    "n_epochs   = 100\n",
    "batch_size = 128\n",
    "x = data.values\n",
    "x = preprocessing.normalize(x)\n",
    "# x=preprocessing.scale(x)\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "y_pred_kmeans = kmeans.fit_predict(x)\n",
    "dims = [x.shape[-1], 500, 500, 1000, 300] \n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "pretrain_epochs = n_epochs\n",
    "batch_size = batch_size\n",
    "save_dir = './results'\n",
    "autoencoder, encoder = autoencoder(dims, init=init)\n",
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)\n",
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "autoencoder.load_weights(save_dir + '/ae_weights.h5')\n",
    "    \n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer=SGD(0.05, 0.9), loss='kld')\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x)).astype(np.float64)\n",
    "y_pred_last = np.copy(y_pred)\n",
    "\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "# computing an auxiliary target distribution\n",
    "\n",
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 1000 # 8000\n",
    "update_interval = 100 # 140\n",
    "index_array = np.arange(x.shape[0])\n",
    "tol = 0.001 # 0.001tolerance threshold to stop training\n",
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "model.load_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "# Eval.\n",
    "q = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)\n",
    "x_embedded = TSNE(n_components=2).fit_transform(x)\n",
    "x_embedded.shape\n",
    "\n",
    "# vis_x = x_embedded[:, 0]\n",
    "# vis_y = x_embedded[:, 1]\n",
    "\n",
    "K.set_floatx('float64')\n",
    "init_1 = VarianceScaling(scale=1. / 3., mode='fan_in',distribution='uniform')\n",
    "autoencoder.load_weights(save_dir + '/ae_weights.h5')\n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=[clustering_layer, autoencoder.output])\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))\n",
    "\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "y_pred_last = np.copy(y_pred)\n",
    "\n",
    "model.compile(loss=['kld', 'mse'], loss_weights=[0.1, 1], optimizer=pretrain_optimizer)\n",
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q, _  = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "\n",
    "        # check stop criterion chnage to float32 for cnv\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=[p[idx], x[idx]])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/b_DEC_model_final.h5')\n",
    "model.load_weights(save_dir + '/b_DEC_model_final.h5')\n",
    "# evaluation of model prediction \n",
    "data = pd.read_csv('newcnv_1000.csv')\n",
    "data.fillna(value=0, inplace = True)\n",
    "data = data.astype(np.float64)\n",
    "x=data.values\n",
    "q, _ = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)\n",
    "savetxt('cnv_train_test_diff.csv', y_pred, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the optimal number of clusters using hypergraph partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fe7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp/ipykernel_1584/1393304641.py:254: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  h_int= [np.int(x) for x in h[i] if x==x]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best overall: 0.006576622021093001 edges left:  15\n",
      "best overall: 0.20908322987511513 edges left:  12\n",
      "best overall: 0.41403131151079786 edges left:  9\n",
      "best overall: 0.6266449781661371 edges left:  6\n",
      "best overall: 0.8461660384380215 edges left:  3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import comb as choose\n",
    "import itertools\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "import collections\n",
    "from scipy.spatial import distance\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "# return: number of nodes (recall: 0-based), and\n",
    "# number of edges of each cardinality\n",
    "def H_size(H):\n",
    "    M = len(H)\n",
    "    m = []\n",
    "    n = 0\n",
    "    for i in range(M):\n",
    "        m.append(len(H[i]))\n",
    "        if(len(H[i]) > 0):\n",
    "            j = max(set.union(*H[i]))\n",
    "\n",
    "            if j > n:\n",
    "                n = j\n",
    "    return n+1, m\n",
    "\n",
    "# vertex d-degrees for each d\n",
    "\n",
    "def d_Degrees(H, n, m):\n",
    "    M = len(H)\n",
    "    d = [[]]*M\n",
    "    for i in range(M):\n",
    "        if(m[i] > 0):\n",
    "            #x = [y for x in H[i] for y in list(x)]\n",
    "            x = [y for x in H[i] for y in list(x)]\n",
    "            y = [x.count(i) for i in range(n)]\n",
    "            d[i] = y\n",
    "    return d\n",
    "\n",
    "# vertex total degrees\n",
    "def Degrees(H, n, m, d):\n",
    "    M = len(H)\n",
    "    D = [0]*n\n",
    "    for i in range(M):\n",
    "        if(m[i] > 0):\n",
    "            for j in range(n):\n",
    "                D[j] = D[j] + d[i][j]\n",
    "    return D\n",
    "\n",
    "##########################################################\n",
    "##entropy calculation\n",
    "def entropy1(labels, base=None):\n",
    "  value,counts = np.unique(labels, return_counts=True)\n",
    "  return entropy(counts, base=base)\n",
    "\n",
    "def conductance_partition(H,A):\n",
    "    print(nx.conductance(H,A))\n",
    "\n",
    "def dist_calc(a,b):\n",
    "    euclidean=0\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            euclidean+=(i-j)**2\n",
    "    return euclidean**0.5\n",
    "\n",
    "######\n",
    "\n",
    "# edge contribution: given (H,A,m)\n",
    "def EdgeContribution(H, A, m):\n",
    "    ec = 0\n",
    "    for i in range(len(H)):\n",
    "        for j in range(len(H[i])):\n",
    "            for k in range(len(A)):\n",
    "                if(H[i][j].issubset(A[k])):\n",
    "                    ec = ec + 1\n",
    "                    break\n",
    "    ec = ec / sum(m)\n",
    "    return ec\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def d_DegreeTax(A, m, d):\n",
    "    dt = 0\n",
    "    for i in range(len(m)):\n",
    "        if (m[i] > 0):\n",
    "            S = 0\n",
    "            for j in range(len(A)):\n",
    "                s = 0\n",
    "                for k in A[j]:\n",
    "                    s = s + d[i][k]\n",
    "                s = s ** i\n",
    "                S = S + s\n",
    "            S = S / (i**i * m[i]**(i-1) * sum(m))\n",
    "            dt = dt + S\n",
    "    return dt\n",
    "\n",
    "# degree tax - with degrees as null model\n",
    "def DegreeTax(A, m, D):\n",
    "    vol = sum(D)\n",
    "    M = sum(m)\n",
    "    # vol(A_i)'s\n",
    "    volA = [0]*len(A)\n",
    "    for i in range(len(A)):\n",
    "        for j in A[i]:\n",
    "            volA[i] = volA[i] + D[j]\n",
    "        volA[i] = volA[i] / vol\n",
    "    # sum over d\n",
    "    S = 0\n",
    "    for i in range(len(m)):\n",
    "        if (m[i] > 0):\n",
    "            x = sum([a**i for a in volA]) * m[i] / M\n",
    "            S = S + x\n",
    "    return S\n",
    "\n",
    "\n",
    "############################\n",
    "def newPart(A, s):\n",
    "    P = []\n",
    "    for i in range(len(A)):\n",
    "        if(len(s.intersection(A[i])) == 0):\n",
    "            P.append(A[i])\n",
    "        else:\n",
    "            s = s.union(A[i])\n",
    "    P.append(s)\n",
    "    return P\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "def cnmAlgo(H,verbose=False, ddeg=False):\n",
    "    # get degrees from H\n",
    "    n, m = H_size(H)\n",
    "    d = d_Degrees(H, n, m)\n",
    "    D = Degrees(H, n, m, d)\n",
    "    # get all edges in a list\n",
    "    e = []\n",
    "    for i in range(len(H)):\n",
    "        e.extend(H[i])\n",
    "    \n",
    "\n",
    "    # initialize modularity, partition\n",
    "    A_opt = []\n",
    "    for i in range(n):\n",
    "        A_opt.extend([{i}])\n",
    "        entropy_value=entropy1(A_opt)\n",
    "           \n",
    "        if (entropy_value !=0):\n",
    "            entropy_log=math.log(entropy_value,2)\n",
    "        else:\n",
    "            entropy_log=0.0001\n",
    "        \n",
    "        q_opt = (EdgeContribution(H, A_opt, m) - DegreeTax(A_opt, m, D))+(entropy_log/len(A_opt)) \n",
    "        \n",
    "       \n",
    "    # e contains the edges NOT yet in a part\n",
    "    while len(e) > 0:\n",
    "        q0 = -1\n",
    "        e0 = -1\n",
    "        if verbose:\n",
    "            print('best overall:', q_opt, 'edges left: ', len(e))\n",
    "        # pick best edge to add .. this is slow as is!\n",
    "        for i in range(len(e)):\n",
    "            P = newPart(A_opt, e[i])\n",
    "            \n",
    "            P_list= list(P)\n",
    "            entropy_value=entropy1(P_list)\n",
    "           \n",
    "            if (entropy_value !=0):\n",
    "                entropy_log=math.log(entropy_value,2)\n",
    "            else:\n",
    "                entropy_log=0.0001\n",
    "            #q = (EdgeContribution(H, P, m) - DegreeTax(P, m, D))*(entropy_log/len(P))\n",
    "            q = (EdgeContribution(H, P, m) - DegreeTax(P, m, D))+(entropy_log/len(P))\n",
    "            \n",
    "            if q > q0:\n",
    "                e0 = i\n",
    "                q0 = q\n",
    "        # add best edge found if any\n",
    "        if(q0 > q_opt):\n",
    "            q_opt = q0\n",
    "            A_opt = newPart(A_opt, e[e0])\n",
    "            # remove all 'active' edges\n",
    "            r = []\n",
    "            for i in range(len(e)):\n",
    "                for j in range(len(A_opt)):\n",
    "                    if(e[i].issubset(A_opt[j])):\n",
    "                        r.append(e[i])\n",
    "                        break\n",
    "            for i in range(len(r)):\n",
    "                e.remove(r[i])\n",
    "        # early stop if no immediate improvement\n",
    "        else:\n",
    "            break\n",
    "    return q_opt, A_opt\n",
    "\n",
    "\n",
    "\n",
    "# Map vertices 0 .. n-1 to their respective 0-based part number\n",
    "\n",
    "\n",
    "def PartitionLabels(P):\n",
    "    n = 0\n",
    "    for i in range(len(P)):\n",
    "        n = n + len(P[i])\n",
    "    label = [-1]*n\n",
    "    for i in range(len(P)):\n",
    "        l = list(P[i])\n",
    "        for j in range(len(l)):\n",
    "            label[l[j]] = i\n",
    "    return label\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# format Hypergraph given list of hyperedges (list of sets of 0-based integers)\n",
    "\n",
    "\n",
    "    \n",
    "def list2H(h):\n",
    "    ml = max([len(x) for x in h])\n",
    "    \n",
    "    H = [[]]*(ml+1)\n",
    "    for i in range(ml+1):\n",
    "        H[i] = []\n",
    "   \n",
    "    for i in range(len(h)):\n",
    "        l = len(h[i])\n",
    "        H[l].append(h[i])\n",
    "    return H\n",
    "\n",
    "# two section modularity\n",
    "\n",
    "\n",
    "def hcutPlus(H,P):\n",
    "    hc = [-1]*len(H)\n",
    "    S = 0\n",
    "    L = 0\n",
    "    for i in range(len(H)):\n",
    "        l = len(H[i])\n",
    "        L = L + l\n",
    "        s = 0\n",
    "        for j in range(len(P)):            \n",
    "            s = s + sum([x < set(P[j]) for x in H[i] ])\n",
    "        S = S + s\n",
    "        if l>0:\n",
    "            hc[i] = (l-s)/l\n",
    "    return hc\n",
    "\n",
    "\n",
    "df =  pd.read_csv('edges_array_newrna.csv',header=None)\n",
    "h = df.apply(set, axis=1)\n",
    "\n",
    "h_int=list()\n",
    "for i in range(len(h)):\n",
    "    h_int= [np.int(x) for x in h[i] if x==x]\n",
    "    h_int=set(h_int)\n",
    "    h[i]=h_int\n",
    "\n",
    "h_2h=list2H(h)\n",
    "qC, PC = cnmAlgo(h_2h,verbose=True)\n",
    "print(len(PC))\n",
    "np.savetxt(\"n5_newrna.csv\",PartitionLabels(PC), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the accuracy of the results by comparing it with the gold standard for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2f2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluate.py...\n",
      "Clustering results file: newrna_200_n5_2.csv\n",
      "gold standard file: ground_test_label2.csv\n",
      "\n",
      "Number of clusters available in the gold standard:  5\n",
      "Number of objects available in the gold standard:  392\n",
      "Number of clusters available in the clustering result:  5\n",
      "Number of objects available in the clustering result:  392\n",
      "Number of objects available in the clustering result that are present in the gold standard: 392\n",
      "\n",
      "Evaluation Results:\n",
      "Precision = 73.21428571428571\n",
      "Recall = 50.0\n",
      "F1-score = 59.42028985507246\n",
      "ARI = 30.99374315281039\n",
      "\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 26\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import scipy.special\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get paths to clustering result and gold standard\n",
    "clustered_file = 'newrna_200_n5_2.csv'\n",
    "gold_standard_file = 'ground_test_label2.csv'\n",
    "\n",
    "print(\"\\nStarting evaluate.py...\")\n",
    "print(\"Clustering results file:\", clustered_file)\n",
    "print(\"gold standard file:\", gold_standard_file)\n",
    "\n",
    "# Get the number of clusters from the gold standard\n",
    "#---------------------------------------------------------\n",
    "gold_standard_n_clusters = 0\n",
    "\n",
    "all_gold_standard_clusters_list = []\n",
    "\n",
    "with open(gold_standard_file) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        all_gold_standard_clusters_list.append(row[1])\n",
    "        \n",
    "gold_standard_clusters_list = list(set(all_gold_standard_clusters_list))\n",
    "gold_standard_n_clusters = len(gold_standard_clusters_list)\n",
    "\n",
    "print(\"\\nNumber of clusters available in the gold standard: \", gold_standard_n_clusters)\n",
    "\n",
    "\n",
    "# Get the gold standard\n",
    "#----------------------------\n",
    "gold_standard_clusters = [[] for x in range(gold_standard_n_clusters)]\n",
    "\n",
    "gold_standard_count = 0\n",
    "\n",
    "with open(gold_standard_file) as contig_clusters:\n",
    "    readCSV = csv.reader(contig_clusters, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        gold_standard_count += 1\n",
    "        contig = row[0]\n",
    "        bin_num = gold_standard_clusters_list.index(row[1])\n",
    "        gold_standard_clusters[bin_num].append(contig)\n",
    "\n",
    "print(\"Number of objects available in the gold standard: \", gold_standard_count)\n",
    "\n",
    "# Get the number of clusters from the initial clustering result\n",
    "#---------------------------------------------------------\n",
    "n_clusters = 0\n",
    "\n",
    "all_clusters_list = []\n",
    "\n",
    "with open(clustered_file) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        all_clusters_list.append(row[1])\n",
    "        \n",
    "clusters_list = list(set(all_clusters_list))\n",
    "n_clusters = len(clusters_list)\n",
    "\n",
    "print(\"Number of clusters available in the clustering result: \", n_clusters)\n",
    "\n",
    "\n",
    "# Get initial clustering result\n",
    "#----------------------------\n",
    "clusters = [[] for x in range(n_clusters)]\n",
    "\n",
    "clustered_count = 0\n",
    "clustered_objects = []\n",
    "\n",
    "with open(clustered_file) as contig_clusters:\n",
    "    readCSV = csv.reader(contig_clusters, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        clustered_count += 1\n",
    "        contig = row[0]\n",
    "        bin_num = clusters_list.index(row[1])\n",
    "        clusters[bin_num].append(contig)\n",
    "        clustered_objects.append(contig)\n",
    "\n",
    "print(\"Number of objects available in the clustering result: \", len(clustered_objects))\n",
    "\n",
    "# Functions to determine precision, recall, F1-score and ARI\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# Get precicion\n",
    "def getPrecision(mat, k, s, total):\n",
    "    sum_k = 0\n",
    "    for i in range(k):\n",
    "        max_s = 0\n",
    "        for j in range(s):\n",
    "            if mat[i][j] > max_s:\n",
    "                max_s = mat[i][j]\n",
    "        sum_k += max_s\n",
    "    return sum_k/total*100\n",
    "\n",
    "# Get recall\n",
    "def getRecall(mat, k, s, total, unclassified):\n",
    "    sum_s = 0\n",
    "    for i in range(s):\n",
    "        max_k = 0\n",
    "        for j in range(k):\n",
    "            if mat[j][i] > max_k:\n",
    "                max_k = mat[j][i]\n",
    "        sum_s += max_k\n",
    "    return sum_s/(total+unclassified)*100\n",
    "\n",
    "# Get ARI\n",
    "def getARI(mat, k, s, N):\n",
    "    t1 = 0    \n",
    "    for i in range(k):\n",
    "        sum_k = 0\n",
    "        for j in range(s):\n",
    "            sum_k += mat[i][j]\n",
    "        t1 += scipy.special.binom(sum_k, 2)\n",
    "        \n",
    "    t2 = 0\n",
    "    for i in range(s):\n",
    "        sum_s = 0\n",
    "        for j in range(k):\n",
    "            sum_s += mat[j][i]\n",
    "        t2 += scipy.special.binom(sum_s, 2)\n",
    "        \n",
    "    t3 = t1*t2/scipy.special.binom(N, 2)\n",
    "    \n",
    "    t = 0\n",
    "    for i in range(k):\n",
    "        for j in range(s):\n",
    "            t += scipy.special.binom(mat[i][j], 2)\n",
    "        \n",
    "    ari = (t-t3)/((t1+t2)/2-t3)*100\n",
    "    return ari\n",
    "\n",
    "# Get F1-score\n",
    "def getF1(prec, recall):\n",
    "    return 2*prec*recall/(prec+recall)\n",
    "\n",
    "\n",
    "# Determine precision, recall, F1-score and ARI for clustering result\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "total_clustered = 0\n",
    "\n",
    "clusters_species = [[0 for x in range(gold_standard_n_clusters)] for y in range(n_clusters)]\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    for j in range(gold_standard_n_clusters):\n",
    "        n = 0\n",
    "        for k in range(clustered_count):\n",
    "            if clustered_objects[k] in clusters[i] and clustered_objects[k] in gold_standard_clusters[j]:\n",
    "                n+=1\n",
    "                total_clustered += 1\n",
    "        clusters_species[i][j] = n\n",
    "\n",
    "print(\"Number of objects available in the clustering result that are present in the gold standard:\", total_clustered)\n",
    "\n",
    "my_precision = getPrecision(clusters_species, n_clusters, gold_standard_n_clusters, total_clustered)\n",
    "my_recall = getRecall(clusters_species, n_clusters, gold_standard_n_clusters, total_clustered, (gold_standard_count-total_clustered))\n",
    "my_ari = getARI(clusters_species, n_clusters, gold_standard_n_clusters, total_clustered)\n",
    "my_f1 = getF1(my_precision, my_recall)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"Precision =\", my_precision)\n",
    "print(\"Recall =\", my_recall)\n",
    "print(\"F1-score =\", my_f1)\n",
    "print(\"ARI =\", my_ari)\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
